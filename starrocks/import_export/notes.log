# Generate parquet dataset from impala-kudu.

## Step 0

### In local, copy scripts to impala server

```
rsync -av /Users/lei/Documents/wsForti/worktmp/starrocks/import_export   root@172.18.3.30:/data2/
```

## Step 1 

### Create external table on impala-kudu host

```
cd /data2/import_export
impala-shell -d db_log_public -f create_parquet_table.sql
```

## Step 2

### 2.1 Check data count in impala-kudu to get what time range is suitable
```
impala-shell -d db_log_public -q "select count(*) from __root_fgt_traffic where itime >= '2025-01-23 16:00:00' and itime<'2025-01-24 16:00:00';"
```


### Or execute in impala-shell `impala-shell -d db_log_public`
```
Query: select count(*) from __root_fgt_traffic where itime >= '2025-01-23 16:00:00' and itime<'2025-01-24 16:00:00';
+------------+
| count(*)   |
+------------+
| 1308913353 |
+------------+
```

### 2.2 Modify the script of `batch_export_dynamic.sh`

```
vi batch_export_dynamic.sh
```

### Change `TIME_RANGE_START` and `TIME_RANGE_END` to the time range in 2.1
```
# ------ Configuration Section ------
# Impala connection
IMPALA_HOST=""
IMPALA_OPTS=""

# Table configuration
SOURCE_TABLE="db_log_public.__root_fgt_traffic"
TARGET_TABLE="db_log_public.dataset_fgt_traffic_parquet"
HDFS_LOCATION="/dataset/parquet/traffic"

# Batch configuration
BATCH_SIZE=10000000  # 10 million rows per batch
TIME_RANGE_START="2025-01-03 06:26:19"
TIME_RANGE_END="2025-01-03 12:17:32"
```


## Step 3

### Run the script of `batch_export_dynamic.sh`

```
nohup ./batch_export_dynamic.sh > nohupoutput.log 2>&1 &
tail -fn 200 nohupoutput.log
```

## Step 4

### Copy parquet files to impala-kudu host. 

( NOTE:Rather than using hdfs dfs -cp as with typical files, we use hadoop distcp -pb to ensure that the special block size of the Parquet data files is preserved. Refer to https://impala.apache.org/docs/build/html/topics/impala_parquet.html)

```
hadoop distcp -pb  hdfs:///dataset file:///data2/

scp -r root@172.18.3.30:/data2/dataset   root@10.105.101.4:/data2/

hadoop distcp -pb  file:///data2/dataset hdfs:///

```
### Check files info on hdfs before copy.
```
hdfs dfs -du  -h /dataset/parquet/traffic
hdfs dfs -du  -h /dataset/parquet
hdfs dfs -du  -h hdfs://cluster/dataset/parquet
```

## Step 5

### Create Starrocks table

```
kubectl exec -it kube-starrocks-fe-0 -- bash
```
```
mysql -h kube-starrocks-fe-0.kube-starrocks-fe-search.default.svc.cluster.local -P 9030 -u root -D db_log_public
```

In `mysql client shell`, run the sql in the file of `create_sr_table.sql` 

## Step 6

### Load data into Starrocks table

In `mysql client shell`, run the sql in the file of `import_sr_pipe.sql` 

### Check the task status
```
show pipes;

SELECT * FROM information_schema.pipe_files  where LOAD_STATE!='UNLOADED' ORDER BY LOAD_STATE;

SELECT count(*) from db_log_public.large_fgt_traffic;
```


================================================================

sample data source: 172.18.3.30

select count(*) from __root_fgt_traffic where itime > '2025-01-23 16:00:00' and itime<='2025-01-24 16:00:00' and id%17=1;

+----------+
| count(*) |
+----------+
| 99304533 |
+----------+

select count(*) from __root_fgt_traffic where itime > '2025-01-23 16:00:00' and itime<='2025-01-24 16:00:00';
+------------+
| count(*)   |
+------------+
| 1688157425 |
+------------+

```
Query: select count(*) from __root_fgt_traffic where itime >= '2025-02-01 00:00:00' and itime<'2025-02-02 00:00:00'
+------------+
| count(*)   |
+------------+
| 1844953328 |
+------------+
```

select count(*) from db_log_public.dataset_fgt_traffic_parquet;
+-----------+
| count(*)  |
+-----------+
| 580929696 |
+-----------+


Query: select count(*) from __root_fgt_traffic  where itime>'2025-01-03 00:00:00' and itime<'2025-01-10 00:00:00'
+-----------+
| count(*)  |
+-----------+
| 573796128 |
+-----------+

rsync -av root@172.18.3.30:/data2/dataset  /Users/lei/Downloads
rsync -av /Users/lei/Downloads/dataset root@10.105.101.4:/data2/
# Or
scp -r root@172.18.3.30:/data2/dataset   root@10.105.101.4:/data2/

select count(*) from db_log_public.large_fgt_traffic ;

SELECT 
      itime, adomid, devid, vd, id, dtime, euid, epid, dsteuid, dstepid,
      logflag, logver, sfsid, logid, type, subtype, `level`, action, utmaction,
      policyid, sessionid, srcip, dstip, tranip, transip, srcport, dstport,
      tranport, transport, trandisp, duration, proto, vrf, slot, sentbyte,
      rcvdbyte, sentdelta, rcvddelta, sentpkt, rcvdpkt, `user`, unauthuser,
      dstunauthuser, srcname, dstname, `group`, service, app, appcat, fctuid,
      srcintfrole, dstintfrole, srcserver, dstserver, appid, appact, apprisk,
      wanoptapptype, policytype, centralnatid, channel, vwpvlanid, shapingpolicyid,
      eventtime, vwlid, shaperdropsentbyte, shaperdroprcvdbyte, shaperperipdropbyte,
      wanin, wanout, lanin, lanout, crscore, craction, crlevel, countapp, countav,
      countdlp, countemail, countips, countweb, countwaf, countssl, countssh,
      countdns, srcuuid, dstuuid, poluuid, srcmac, mastersrcmac, dstmac,
      masterdstmac, srchwvendor, srchwversion, srcfamily, srcswversion,
      dsthwvendor, dsthwversion, dstfamily, dstswversion, devtype, devcategory,
      dstdevtype, dstdevcategory, osname, osversion, dstosname, dstosversion,
      srccountry, dstcountry, srcssid, dstssid, srcintf, dstintf, srcinetsvc,
      dstinetsvc, unauthusersource, dstunauthusersource, authserver, applist,
      vpn, vpntype, radioband, policyname, policymode, sslaction, url, agent,
      `comment`, ap, apsn, vwlservice, vwlquality, collectedemail, dstcollectedemail,
      shapersentname, shaperrcvdname, shaperperipname, msg, custom_field1,
      utmevent, utmsubtype, sender, recipient, virus, attack, hostname, catdesc,
      dlpsensor, utmref, tdinfoid, dstowner, tdtype, tdscantime, tdthreattype,
      tdthreatname, tdwfcate, threatwgts, threatcnts, threatlvls, saasinfo, ebtime,
      clouduser, threats, threattyps, apps, countff, identifier, securityid,
      securityact, tz, srcdomain, counticap, dstregion, srcregion, dstcity,
      srccity, `signal`, snr, dstauthserver, dstgroup, dstuser, tunnelid, vwlname,
      srcthreatfeed, dstthreatfeed, psrcport, pdstport, srcreputation, dstreputation,
      vip, accessproxy, gatewayid, clientdeviceid, clientdeviceowner, clientdevicetags,
      httpmethod, referralurl, saasname, srcmacvendor, shapingpolicyname, accessctrl,
      countcifs, proxyapptype, clientdevicemanageable, emsconnection, srcremote,
      replydstintf, replysrcintf, vsn, countsctpf, realserverid, clientdeviceems,
      clientcert, countcasb, durationdelta, countvpatch, sentpktdelta, rcvdpktdelta,
      fwdsrv
FROM FILES
(
    "path" = "hdfs://198.18.1.10/dataset/parquet/traffic/*",
    "format" = "parquet",
    "hadoop.security.authentication" = "simple",
    "username" = "root",
    "password" = "fortinet@123"
)
limit 3
; 

SELECT 
      count(*)
FROM FILES
(
    "path" = "hdfs://198.18.1.10/dataset/parquet/traffic/*",
    "format" = "parquet",
    "hadoop.security.authentication" = "simple",
    "username" = "root",
    "password" = "fortinet@123"
)
limit 3
; 